{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"WEB MINING \u00b6 NAMA : Hidayatullah NIM : 180411100131 PEMBIMBING : @mulaab MATERIAL \u00b6 Crawling Data Preprocessing Data TF-IDF Modelling Data","title":"WEB MINING"},{"location":"#web_mining","text":"NAMA : Hidayatullah NIM : 180411100131 PEMBIMBING : @mulaab","title":"WEB MINING"},{"location":"#material","text":"Crawling Data Preprocessing Data TF-IDF Modelling Data","title":"MATERIAL"},{"location":"Crawling-Data/","text":"Crawling Data (materi 1) \u00b6 Sebenarnya untuk mendapatkan data twitter ada beberapa cara, di antaranya Scraping & Crawling. Dan pada saat tulisan ini dibuat untuk Scraping data twitter sudah tidak dapat dilakukan, hal itu dikarenakan selector class dan id HTML di Twitter selalu berubah-ubah. Crawling Menggunakan Python \u00b6 Untuk crawling data twitter kita akan menggunakan library Tweepy . Untuk menginstall Tweepy via pip pip install tweepy Sebagai contoh kita ingin crawling data twitter dengan kata kunci python* dengan tweet yang dibuat pada antara tanggal 01 januari 2021 s/d 10 januari 2021 dengan maksimal jumlah tweet adalah 100, kita juga akan exluced retweet,* sehingga retweet tidak akan di tampilkan. Lalu hasil crawling akan disimpan dengan format .csv import tweepy import csv access_token = \"\" access_token_secret = \"\" consumer_key = \"\" consumer_key_secret = \"\" auth = tweepy . OAuthHandler ( consumer_key , consumer_key_secret ) api = tweepy . API ( auth ) # Open/create a file to append data to csvFile = open ( 'nama-file.csv' , 'w' , encoding = 'utf-8' ) #Use csv writer csvWriter = csv . writer ( csvFile ) for tweet in tweepy . Cursor ( api . search , q = '#Python -filter:retweets' , tweet_mode = 'extended' , lang = \"id\" , since = '2021-01-01' , until = '2021-01-10' ) . items ( 100 ): text = tweet . full_text user = tweet . user . name created = tweet . created_at csvWriter . writerow ([ created , text . encode ( 'utf-8' ), user ]) csvWriter = csv . writer ( csvFile ) csvFile . close ()","title":"Crawling Data (Materi 1)"},{"location":"Crawling-Data/#crawling_data_materi_1","text":"Sebenarnya untuk mendapatkan data twitter ada beberapa cara, di antaranya Scraping & Crawling. Dan pada saat tulisan ini dibuat untuk Scraping data twitter sudah tidak dapat dilakukan, hal itu dikarenakan selector class dan id HTML di Twitter selalu berubah-ubah.","title":"Crawling Data (materi 1)"},{"location":"Crawling-Data/#crawling_menggunakan_python","text":"Untuk crawling data twitter kita akan menggunakan library Tweepy . Untuk menginstall Tweepy via pip pip install tweepy Sebagai contoh kita ingin crawling data twitter dengan kata kunci python* dengan tweet yang dibuat pada antara tanggal 01 januari 2021 s/d 10 januari 2021 dengan maksimal jumlah tweet adalah 100, kita juga akan exluced retweet,* sehingga retweet tidak akan di tampilkan. Lalu hasil crawling akan disimpan dengan format .csv import tweepy import csv access_token = \"\" access_token_secret = \"\" consumer_key = \"\" consumer_key_secret = \"\" auth = tweepy . OAuthHandler ( consumer_key , consumer_key_secret ) api = tweepy . API ( auth ) # Open/create a file to append data to csvFile = open ( 'nama-file.csv' , 'w' , encoding = 'utf-8' ) #Use csv writer csvWriter = csv . writer ( csvFile ) for tweet in tweepy . Cursor ( api . search , q = '#Python -filter:retweets' , tweet_mode = 'extended' , lang = \"id\" , since = '2021-01-01' , until = '2021-01-10' ) . items ( 100 ): text = tweet . full_text user = tweet . user . name created = tweet . created_at csvWriter . writerow ([ created , text . encode ( 'utf-8' ), user ]) csvWriter = csv . writer ( csvFile ) csvFile . close ()","title":"Crawling Menggunakan Python"},{"location":"Modelling/","text":"Modelling (materi 4) \u00b6 Pemodelan pengguna ( User Modeling ) merupakan subdivisi dari interkaso manusia dan komputer yang menggambarkan proses membangun dan memodifikasi pemahaman konseptual pengguna dimana tujuan dari pemodelan ini adalah penyesuaian dan adaptasi sistem dengan kebutuhan spesifik dari pengguna Cara Kerja Algoritma \u00b6 Dalam Algoritma KNN, K adalah jumlah tetangga terdekat, dimana jumlah tetangga dalah faktor penentu inti. K umumnya merupakan bilangan ganjil jika jumlah kelas adalah 2, dan bila K = 1 maka algoritma tersebut dikenal dengan nama algoritma tetangga terdekat. Algoritma K-NN memiliki tahaban sebagai berikut : Menghitung Jarak Menentukan tetangga terdekat Memberi suara untuk label Load Data Set \u00b6 import pandas as pd import numpy as np #Import scikit-learn dataset library from sklearn import datasets #Load dataset wine = pd . read_csv ( \"TF-IDF.csv\" ) wine . head () Memisahkan Data \u00b6 Mari kita pisahkan dataset dengan menggunakan fungsi train_test_split(). Anda harus melewati 3 parameter fitur, target, dan ukuran test_set. Selain itu, Anda dapat menggunakan random_state untuk memilih catatan secara acak. # Import train_test_split function from sklearn.model_selection import train_test_split # Split dataset into training set and test set X_train , X_test , y_train , y_test = train_test_split ( wine . data , wine . target , test_size = 0.3 ) # 70% training and 30% test Model Untuk K = 5 \u00b6 #Import knearest neighbors Classifier model from sklearn.neighbors import KNeighborsClassifier #Create KNN Classifier knn = KNeighborsClassifier ( n_neighbors = 5 ) #Train the model using the training sets knn . fit ( X_train , y_train ) #Predict the response for test dataset y_pred = knn . predict ( X_test ) Evaluasi Model untuk K = 5 \u00b6 #Import scikit-learn metrics module for accuracy calculation from sklearn import metrics # Model Accuracy, how often is the classifier correct? print ( \"Accuracy:\" , metrics . accuracy_score ( y_test , y_pred ))","title":"Modelling (Materi 4)"},{"location":"Modelling/#modelling_materi_4","text":"Pemodelan pengguna ( User Modeling ) merupakan subdivisi dari interkaso manusia dan komputer yang menggambarkan proses membangun dan memodifikasi pemahaman konseptual pengguna dimana tujuan dari pemodelan ini adalah penyesuaian dan adaptasi sistem dengan kebutuhan spesifik dari pengguna","title":"Modelling (materi 4)"},{"location":"Modelling/#cara_kerja_algoritma","text":"Dalam Algoritma KNN, K adalah jumlah tetangga terdekat, dimana jumlah tetangga dalah faktor penentu inti. K umumnya merupakan bilangan ganjil jika jumlah kelas adalah 2, dan bila K = 1 maka algoritma tersebut dikenal dengan nama algoritma tetangga terdekat. Algoritma K-NN memiliki tahaban sebagai berikut : Menghitung Jarak Menentukan tetangga terdekat Memberi suara untuk label","title":"Cara Kerja Algoritma"},{"location":"Modelling/#load_data_set","text":"import pandas as pd import numpy as np #Import scikit-learn dataset library from sklearn import datasets #Load dataset wine = pd . read_csv ( \"TF-IDF.csv\" ) wine . head ()","title":"Load Data Set"},{"location":"Modelling/#memisahkan_data","text":"Mari kita pisahkan dataset dengan menggunakan fungsi train_test_split(). Anda harus melewati 3 parameter fitur, target, dan ukuran test_set. Selain itu, Anda dapat menggunakan random_state untuk memilih catatan secara acak. # Import train_test_split function from sklearn.model_selection import train_test_split # Split dataset into training set and test set X_train , X_test , y_train , y_test = train_test_split ( wine . data , wine . target , test_size = 0.3 ) # 70% training and 30% test","title":"Memisahkan Data"},{"location":"Modelling/#model_untuk_k_5","text":"#Import knearest neighbors Classifier model from sklearn.neighbors import KNeighborsClassifier #Create KNN Classifier knn = KNeighborsClassifier ( n_neighbors = 5 ) #Train the model using the training sets knn . fit ( X_train , y_train ) #Predict the response for test dataset y_pred = knn . predict ( X_test )","title":"Model Untuk K = 5"},{"location":"Modelling/#evaluasi_model_untuk_k_5","text":"#Import scikit-learn metrics module for accuracy calculation from sklearn import metrics # Model Accuracy, how often is the classifier correct? print ( \"Accuracy:\" , metrics . accuracy_score ( y_test , y_pred ))","title":"Evaluasi Model untuk K = 5"},{"location":"TF-IDF/","text":"TF-IDF (materi3) \u00b6 TF-IDF adalah ukuran statistik yang menggambarkan pentingnya suatu istilah terhadap sebuah dokumen dalam sebuah kumpulan atau korpus 1. ukuran ini sering digunakan sebagai faktor pembobot dalam pencarian temu balik informasi (information retrieval), penambangan text (text mining), dan pemodelan pengguna (user-modeling). Dalam TF-IDF Term frequency menyatakan fekuensi (tingkat keseringan) munculnya seuatu term dalam dokumen, sedangkan document frequency merupakan banyaknya jumlah dokumen dimana sebuah term itu muncul2. Proses TF-IDF \u00b6 Siapakan data yang akan diolah, baca file menggunakan Pandas. Kolom yang dibutuhkan dalam pemrosesan kali ini adalah kolom label dan tweet_tokens_stemmed . Kemudian rename kolom pada Dataframe menjadi [\"label\", \"tweet\"] import pandas as pd import numpy as np TWEET_DATA = pd . read_csv ( \"Text_Preprocessing.csv\" , usecols = [ \"label\" , \"tweet_tokens_stemmed\" ]) TWEET_DATA . columns = [ \"label\" , \"tweet\" ] TWEET_DATA . head () # convert list formated string to list import ast def convert_text_list ( texts ): texts = ast . literal_eval ( texts ) return [ text for text in texts ] TWEET_DATA [ \"tweet_list\" ] = TWEET_DATA [ \"tweet\" ] . apply ( convert_text_list ) print ( TWEET_DATA [ \"tweet_list\" ][ 90 ]) print ( \" \\n type : \" , type ( TWEET_DATA [ \"tweet_list\" ][ 90 ])) # convert list formated string to list Jika ketika kita cek, item di kolom tweet memiliki tipe data <class str> , # maka kita perlu mengubah string menjadi list dan menyimpan pada Pandas Seriec tweet_list , import ast def convert_text_list ( texts ): texts = ast . literal_eval ( texts ) return [ text for text in texts ] TWEET_DATA [ \"tweet_list\" ] = TWEET_DATA [ \"tweet\" ] . apply ( convert_text_list ) print ( TWEET_DATA [ \"tweet_list\" ][ 90 ]) print ( \" \\n type : \" , type ( TWEET_DATA [ \"tweet_list\" ][ 90 ])) Term Frequency (TF) \u00b6 Setelah me-load data diatas selanjutnya membuat TF ( Term Frequency ), dimana yang sudah dijelaskan diatas Term Frequency adlah frekuensi kemunculan term i pada dokumen j dibagi dengan total dari term pada documen j . def calc_TF ( document ): # Counts the number of times the word appears in review TF_dict = {} for term in document : if term in TF_dict : TF_dict [ term ] += 1 else : TF_dict [ term ] = 1 # Computes tf for each word for term in TF_dict : TF_dict [ term ] = TF_dict [ term ] / len ( document ) return TF_dict TWEET_DATA [ \"TF_dict\" ] = TWEET_DATA [ 'tweet_list' ] . apply ( calc_TF ) TWEET_DATA [ \"TF_dict\" ] . head () # Check TF result index = 90 print ( ' %20s ' % \"term\" , \" \\t \" , \"TF \\n \" ) for key in TWEET_DATA [ \"TF_dict\" ][ index ]: print ( ' %20s ' % key , \" \\t \" , TWEET_DATA [ \"TF_dict\" ][ index ][ key ]) Inverse Document Frequency (IDF) \u00b6 IDF berfungsi mengurangi bobot suatu term jika sering muncul dan tersebar diseruluh dokumen, kemudian IDF dapat dituliskan dalam persamaan idf(t,D)=log\u2061N|{d\u2208D:t\u2208d}| Dimana N menunjukan jumlah total dokumen dalam suatu corpus, N = |D|. |{d \u2208 D : t \u2208 d}| = df(t), merupakan jumlah dokumen yang mengandung term t . Dalam bentuk yang lain, IDF dapat dituliskan dengan persamaan, idf(t,D)=log\u22c5\u2061(Ndf(t)+1) Penambahan 1 untuk menghidarkan pembagian terhadap 0 jika df(t) tidak ditemukan di dalam corpus. def calc_DF ( tfDict ): count_DF = {} # Run through each document's tf dictionary and increment countDict's (term, doc) pair for document in tfDict : for term in document : if term in count_DF : count_DF [ term ] += 1 else : count_DF [ term ] = 1 return count_DF DF = calc_DF ( TWEET_DATA [ \"TF_dict\" ]) n_document = len ( TWEET_DATA ) def calc_IDF ( __n_document , __DF ): IDF_Dict = {} for term in __DF : IDF_Dict [ term ] = np . log ( __n_document / ( __DF [ term ] + 1 )) return IDF_Dict #Stores the idf dictionary IDF = calc_IDF ( n_document , DF ) #calc TF-IDF def calc_TF_IDF ( TF ): TF_IDF_Dict = {} #For each word in the review, we multiply its tf and its idf. for key in TF : TF_IDF_Dict [ key ] = TF [ key ] * IDF [ key ] return TF_IDF_Dict #Stores the TF-IDF Series TWEET_DATA [ \"TF-IDF_dict\" ] = TWEET_DATA [ \"TF_dict\" ] . apply ( calc_TF_IDF ) # Check TF-IDF result index = 90 print ( ' %20s ' % \"term\" , \" \\t \" , ' %10s ' % \"TF\" , \" \\t \" , ' %20s ' % \"TF-IDF \\n \" ) for key in TWEET_DATA [ \"TF-IDF_dict\" ][ index ]: print ( ' %20s ' % key , \" \\t \" , TWEET_DATA [ \"TF_dict\" ][ index ][ key ] , \" \\t \" , TWEET_DATA [ \"TF-IDF_dict\" ][ index ][ key ]) # sort descending by value for DF dictionary sorted_DF = sorted ( DF . items (), key = lambda kv : kv [ 1 ], reverse = True )[: 50 ] # Create a list of unique words from sorted dictionay `sorted_DF` unique_term = [ item [ 0 ] for item in sorted_DF ] def calc_TF_IDF_Vec ( __TF_IDF_Dict ): TF_IDF_vector = [ 0.0 ] * len ( unique_term ) # For each unique word, if it is in the review, store its TF-IDF value. for i , term in enumerate ( unique_term ): if term in __TF_IDF_Dict : TF_IDF_vector [ i ] = __TF_IDF_Dict [ term ] return TF_IDF_vector TWEET_DATA [ \"TF_IDF_Vec\" ] = TWEET_DATA [ \"TF-IDF_dict\" ] . apply ( calc_TF_IDF_Vec ) print ( \"print first row matrix TF_IDF_Vec Series \\n \" ) print ( TWEET_DATA [ \"TF_IDF_Vec\" ][ 0 ]) print ( \" \\n matrix size : \" , len ( TWEET_DATA [ \"TF_IDF_Vec\" ][ 0 ])) # Convert Series to List TF_IDF_Vec_List = np . array ( TWEET_DATA [ \"TF_IDF_Vec\" ] . to_list ()) # Sum element vector in axis=0 sums = TF_IDF_Vec_List . sum ( axis = 0 ) data = [] for col , term in enumerate ( unique_term ): data . append (( term , sums [ col ])) ranking = pd . DataFrame ( data , columns = [ 'term' , 'rank' ]) ranking . sort_values ( 'rank' , ascending = False ) Save TF-IDF Data \u00b6 Save ke CSV \u00b6 TWEET_DATA.to_csv(\"Text_TF-IDF.csv\") Save ke Excel \u00b6 TWEET_DATA.to_excel(\"Text_TF-IDF.xlsx\") Save ke HDF5 \u00b6 TWEET_DATA.to_hdf(\"Text_TF-IDF.h5\")","title":"TF-IDF (Materi 3)"},{"location":"TF-IDF/#tf-idf_materi3","text":"TF-IDF adalah ukuran statistik yang menggambarkan pentingnya suatu istilah terhadap sebuah dokumen dalam sebuah kumpulan atau korpus 1. ukuran ini sering digunakan sebagai faktor pembobot dalam pencarian temu balik informasi (information retrieval), penambangan text (text mining), dan pemodelan pengguna (user-modeling). Dalam TF-IDF Term frequency menyatakan fekuensi (tingkat keseringan) munculnya seuatu term dalam dokumen, sedangkan document frequency merupakan banyaknya jumlah dokumen dimana sebuah term itu muncul2.","title":"TF-IDF (materi3)"},{"location":"TF-IDF/#proses_tf-idf","text":"Siapakan data yang akan diolah, baca file menggunakan Pandas. Kolom yang dibutuhkan dalam pemrosesan kali ini adalah kolom label dan tweet_tokens_stemmed . Kemudian rename kolom pada Dataframe menjadi [\"label\", \"tweet\"] import pandas as pd import numpy as np TWEET_DATA = pd . read_csv ( \"Text_Preprocessing.csv\" , usecols = [ \"label\" , \"tweet_tokens_stemmed\" ]) TWEET_DATA . columns = [ \"label\" , \"tweet\" ] TWEET_DATA . head () # convert list formated string to list import ast def convert_text_list ( texts ): texts = ast . literal_eval ( texts ) return [ text for text in texts ] TWEET_DATA [ \"tweet_list\" ] = TWEET_DATA [ \"tweet\" ] . apply ( convert_text_list ) print ( TWEET_DATA [ \"tweet_list\" ][ 90 ]) print ( \" \\n type : \" , type ( TWEET_DATA [ \"tweet_list\" ][ 90 ])) # convert list formated string to list Jika ketika kita cek, item di kolom tweet memiliki tipe data <class str> , # maka kita perlu mengubah string menjadi list dan menyimpan pada Pandas Seriec tweet_list , import ast def convert_text_list ( texts ): texts = ast . literal_eval ( texts ) return [ text for text in texts ] TWEET_DATA [ \"tweet_list\" ] = TWEET_DATA [ \"tweet\" ] . apply ( convert_text_list ) print ( TWEET_DATA [ \"tweet_list\" ][ 90 ]) print ( \" \\n type : \" , type ( TWEET_DATA [ \"tweet_list\" ][ 90 ]))","title":"Proses TF-IDF"},{"location":"TF-IDF/#term_frequency_tf","text":"Setelah me-load data diatas selanjutnya membuat TF ( Term Frequency ), dimana yang sudah dijelaskan diatas Term Frequency adlah frekuensi kemunculan term i pada dokumen j dibagi dengan total dari term pada documen j . def calc_TF ( document ): # Counts the number of times the word appears in review TF_dict = {} for term in document : if term in TF_dict : TF_dict [ term ] += 1 else : TF_dict [ term ] = 1 # Computes tf for each word for term in TF_dict : TF_dict [ term ] = TF_dict [ term ] / len ( document ) return TF_dict TWEET_DATA [ \"TF_dict\" ] = TWEET_DATA [ 'tweet_list' ] . apply ( calc_TF ) TWEET_DATA [ \"TF_dict\" ] . head () # Check TF result index = 90 print ( ' %20s ' % \"term\" , \" \\t \" , \"TF \\n \" ) for key in TWEET_DATA [ \"TF_dict\" ][ index ]: print ( ' %20s ' % key , \" \\t \" , TWEET_DATA [ \"TF_dict\" ][ index ][ key ])","title":"Term Frequency (TF)"},{"location":"TF-IDF/#inverse_document_frequency_idf","text":"IDF berfungsi mengurangi bobot suatu term jika sering muncul dan tersebar diseruluh dokumen, kemudian IDF dapat dituliskan dalam persamaan idf(t,D)=log\u2061N|{d\u2208D:t\u2208d}| Dimana N menunjukan jumlah total dokumen dalam suatu corpus, N = |D|. |{d \u2208 D : t \u2208 d}| = df(t), merupakan jumlah dokumen yang mengandung term t . Dalam bentuk yang lain, IDF dapat dituliskan dengan persamaan, idf(t,D)=log\u22c5\u2061(Ndf(t)+1) Penambahan 1 untuk menghidarkan pembagian terhadap 0 jika df(t) tidak ditemukan di dalam corpus. def calc_DF ( tfDict ): count_DF = {} # Run through each document's tf dictionary and increment countDict's (term, doc) pair for document in tfDict : for term in document : if term in count_DF : count_DF [ term ] += 1 else : count_DF [ term ] = 1 return count_DF DF = calc_DF ( TWEET_DATA [ \"TF_dict\" ]) n_document = len ( TWEET_DATA ) def calc_IDF ( __n_document , __DF ): IDF_Dict = {} for term in __DF : IDF_Dict [ term ] = np . log ( __n_document / ( __DF [ term ] + 1 )) return IDF_Dict #Stores the idf dictionary IDF = calc_IDF ( n_document , DF ) #calc TF-IDF def calc_TF_IDF ( TF ): TF_IDF_Dict = {} #For each word in the review, we multiply its tf and its idf. for key in TF : TF_IDF_Dict [ key ] = TF [ key ] * IDF [ key ] return TF_IDF_Dict #Stores the TF-IDF Series TWEET_DATA [ \"TF-IDF_dict\" ] = TWEET_DATA [ \"TF_dict\" ] . apply ( calc_TF_IDF ) # Check TF-IDF result index = 90 print ( ' %20s ' % \"term\" , \" \\t \" , ' %10s ' % \"TF\" , \" \\t \" , ' %20s ' % \"TF-IDF \\n \" ) for key in TWEET_DATA [ \"TF-IDF_dict\" ][ index ]: print ( ' %20s ' % key , \" \\t \" , TWEET_DATA [ \"TF_dict\" ][ index ][ key ] , \" \\t \" , TWEET_DATA [ \"TF-IDF_dict\" ][ index ][ key ]) # sort descending by value for DF dictionary sorted_DF = sorted ( DF . items (), key = lambda kv : kv [ 1 ], reverse = True )[: 50 ] # Create a list of unique words from sorted dictionay `sorted_DF` unique_term = [ item [ 0 ] for item in sorted_DF ] def calc_TF_IDF_Vec ( __TF_IDF_Dict ): TF_IDF_vector = [ 0.0 ] * len ( unique_term ) # For each unique word, if it is in the review, store its TF-IDF value. for i , term in enumerate ( unique_term ): if term in __TF_IDF_Dict : TF_IDF_vector [ i ] = __TF_IDF_Dict [ term ] return TF_IDF_vector TWEET_DATA [ \"TF_IDF_Vec\" ] = TWEET_DATA [ \"TF-IDF_dict\" ] . apply ( calc_TF_IDF_Vec ) print ( \"print first row matrix TF_IDF_Vec Series \\n \" ) print ( TWEET_DATA [ \"TF_IDF_Vec\" ][ 0 ]) print ( \" \\n matrix size : \" , len ( TWEET_DATA [ \"TF_IDF_Vec\" ][ 0 ])) # Convert Series to List TF_IDF_Vec_List = np . array ( TWEET_DATA [ \"TF_IDF_Vec\" ] . to_list ()) # Sum element vector in axis=0 sums = TF_IDF_Vec_List . sum ( axis = 0 ) data = [] for col , term in enumerate ( unique_term ): data . append (( term , sums [ col ])) ranking = pd . DataFrame ( data , columns = [ 'term' , 'rank' ]) ranking . sort_values ( 'rank' , ascending = False )","title":"Inverse Document Frequency (IDF)"},{"location":"TF-IDF/#save_tf-idf_data","text":"","title":"Save TF-IDF Data"},{"location":"TF-IDF/#save_ke_csv","text":"TWEET_DATA.to_csv(\"Text_TF-IDF.csv\")","title":"Save ke CSV"},{"location":"TF-IDF/#save_ke_excel","text":"TWEET_DATA.to_excel(\"Text_TF-IDF.xlsx\")","title":"Save ke Excel"},{"location":"TF-IDF/#save_ke_hdf5","text":"TWEET_DATA.to_hdf(\"Text_TF-IDF.h5\")","title":"Save ke HDF5"},{"location":"license/","text":"License \u00b6 MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY Support Author \u00b6 Amazon wish list","title":"License"},{"location":"license/#license","text":"MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY","title":"License"},{"location":"license/#support_author","text":"Amazon wish list","title":"Support Author"},{"location":"material-for-mkdocs/","text":"Material for MkDocs \u00b6 MkDocs \u00b6 mkdocs/mkdocs: Project documentation with Markdown - GitHub Material for MkDocs \u00b6 squidfunk/mkdocs-material: A Material Design theme for MkDocs","title":"Material for MkDocs"},{"location":"material-for-mkdocs/#material_for_mkdocs","text":"","title":"Material for MkDocs"},{"location":"material-for-mkdocs/#mkdocs","text":"mkdocs/mkdocs: Project documentation with Markdown - GitHub","title":"MkDocs"},{"location":"material-for-mkdocs/#material_for_mkdocs_1","text":"squidfunk/mkdocs-material: A Material Design theme for MkDocs","title":"Material for MkDocs"},{"location":"extensions/code-hilite/","text":"CodeHilite \u00b6 CodeHilite - Material for MkDocs Supported languages - Pygments Configure mkdocs.yml \u00b6 markdown_extensions: - codehilite","title":"CodeHilite"},{"location":"extensions/code-hilite/#codehilite","text":"CodeHilite - Material for MkDocs Supported languages - Pygments","title":"CodeHilite"},{"location":"extensions/code-hilite/#configure_mkdocsyml","text":"markdown_extensions: - codehilite","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/","text":"Footnote \u00b6 Footnotes - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - footnotes Example \u00b6 Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Footnote"},{"location":"extensions/footnote/#footnote","text":"Footnotes - Material for MkDocs","title":"Footnote"},{"location":"extensions/footnote/#configure_mkdocsyml","text":"markdown_extensions: - footnotes","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/#example","text":"Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Example"},{"location":"extensions/mathjax/","text":"MathJax \u00b6 PyMdown - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - mdx_math: enable_dollar_delimiter: True Example code \u00b6 $$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$ Example rendering \u00b6 P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha","title":"MathJax"},{"location":"extensions/mathjax/#mathjax","text":"PyMdown - Material for MkDocs","title":"MathJax"},{"location":"extensions/mathjax/#configure_mkdocsyml","text":"markdown_extensions: - mdx_math: enable_dollar_delimiter: True","title":"Configure mkdocs.yml"},{"location":"extensions/mathjax/#example_code","text":"$$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$","title":"Example code"},{"location":"extensions/mathjax/#example_rendering","text":"P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha","title":"Example rendering"}]}