{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"WEB MINING \u00b6 NAMA : Hidayatullah NIM : 180411100131 PEMBIMBING : @mulaab MATERIAL \u00b6 Crawling Data Preprocessing Data TF-IDF Modelling Data","title":"WEB MINING"},{"location":"#web_mining","text":"NAMA : Hidayatullah NIM : 180411100131 PEMBIMBING : @mulaab","title":"WEB MINING"},{"location":"#material","text":"Crawling Data Preprocessing Data TF-IDF Modelling Data","title":"MATERIAL"},{"location":"Crawling-Data/","text":"Crawling Data (materi 1) \u00b6 Sebenarnya untuk mendapatkan data twitter ada beberapa cara, di antaranya Scraping & Crawling. Dan pada saat tulisan ini dibuat untuk Scraping data twitter sudah tidak dapat dilakukan, hal itu dikarenakan selector class dan id HTML di Twitter selalu berubah-ubah. Crawling Menggunakan Python \u00b6 Untuk crawling data twitter kita akan menggunakan library Tweepy . Untuk menginstall Tweepy via pip pip install tweepy Sebagai contoh kita ingin crawling data twitter dengan kata kunci python* dengan tweet yang dibuat pada antara tanggal 01 januari 2021 s/d 10 januari 2021 dengan maksimal jumlah tweet adalah 100, kita juga akan exluced retweet,* sehingga retweet tidak akan di tampilkan. Lalu hasil crawling akan disimpan dengan format .csv import tweepy import csv access_token = \"\" access_token_secret = \"\" consumer_key = \"\" consumer_key_secret = \"\" auth = tweepy . OAuthHandler ( consumer_key , consumer_key_secret ) api = tweepy . API ( auth ) # Open/create a file to append data to csvFile = open ( 'nama-file.csv' , 'w' , encoding = 'utf-8' ) #Use csv writer csvWriter = csv . writer ( csvFile ) for tweet in tweepy . Cursor ( api . search , q = '#Python -filter:retweets' , tweet_mode = 'extended' , lang = \"id\" , since = '2021-01-01' , until = '2021-01-10' ) . items ( 100 ): text = tweet . full_text user = tweet . user . name created = tweet . created_at csvWriter . writerow ([ created , text . encode ( 'utf-8' ), user ]) csvWriter = csv . writer ( csvFile ) csvFile . close ()","title":"Crawling Data (Materi 1)"},{"location":"Crawling-Data/#crawling_data_materi_1","text":"Sebenarnya untuk mendapatkan data twitter ada beberapa cara, di antaranya Scraping & Crawling. Dan pada saat tulisan ini dibuat untuk Scraping data twitter sudah tidak dapat dilakukan, hal itu dikarenakan selector class dan id HTML di Twitter selalu berubah-ubah.","title":"Crawling Data (materi 1)"},{"location":"Crawling-Data/#crawling_menggunakan_python","text":"Untuk crawling data twitter kita akan menggunakan library Tweepy . Untuk menginstall Tweepy via pip pip install tweepy Sebagai contoh kita ingin crawling data twitter dengan kata kunci python* dengan tweet yang dibuat pada antara tanggal 01 januari 2021 s/d 10 januari 2021 dengan maksimal jumlah tweet adalah 100, kita juga akan exluced retweet,* sehingga retweet tidak akan di tampilkan. Lalu hasil crawling akan disimpan dengan format .csv import tweepy import csv access_token = \"\" access_token_secret = \"\" consumer_key = \"\" consumer_key_secret = \"\" auth = tweepy . OAuthHandler ( consumer_key , consumer_key_secret ) api = tweepy . API ( auth ) # Open/create a file to append data to csvFile = open ( 'nama-file.csv' , 'w' , encoding = 'utf-8' ) #Use csv writer csvWriter = csv . writer ( csvFile ) for tweet in tweepy . Cursor ( api . search , q = '#Python -filter:retweets' , tweet_mode = 'extended' , lang = \"id\" , since = '2021-01-01' , until = '2021-01-10' ) . items ( 100 ): text = tweet . full_text user = tweet . user . name created = tweet . created_at csvWriter . writerow ([ created , text . encode ( 'utf-8' ), user ]) csvWriter = csv . writer ( csvFile ) csvFile . close ()","title":"Crawling Menggunakan Python"},{"location":"TF-IDF/","text":"TF-IDF (materi3) \u00b6 TF-IDF adalah ukuran statistik yang menggambarkan pentingnya suatu istilah terhadap sebuah dokumen dalam sebuah kumpulan atau korpus 1. ukuran ini sering digunakan sebagai faktor pembobot dalam pencarian temu balik informasi (information retrieval), penambangan text (text mining), dan pemodelan pengguna (user-modeling). Dalam TF-IDF Term frequency menyatakan fekuensi (tingkat keseringan) munculnya seuatu term dalam dokumen, sedangkan document frequency merupakan banyaknya jumlah dokumen dimana sebuah term itu muncul2. Proses TF-IDF \u00b6 Siapakan data yang akan diolah, baca file menggunakan Pandas. Kolom yang dibutuhkan dalam pemrosesan kali ini adalah kolom label dan tweet_tokens_stemmed . Kemudian rename kolom pada Dataframe menjadi [\"label\", \"tweet\"] ```import pandas as pd import pandas as pd import numpy as np TWEET_DATA = pd.read_csv(\"Text_Preprocessing.csv\", usecols=[\"label\", \"tweet_tokens_stemmed\"]) TWEET_DATA.columns = [\"label\", \"tweet\"] TWEET_DATA.head() convert list formated string to list \u00b6 import ast def convert_text_list(texts): texts = ast.literal_eval(texts) return [text for text in texts] TWEET_DATA[\"tweet_list\"] = TWEET_DATA[\"tweet\"].apply(convert_text_list) print(TWEET_DATA[\"tweet_list\"][90]) print(\"\\ntype : \", type(TWEET_DATA[\"tweet_list\"][90])) convert list formated string to list Jika ketika kita cek, item di kolom tweet memiliki tipe data , \u00b6 maka kita perlu mengubah string menjadi list dan menyimpan pada Pandas Seriec tweet_list , \u00b6 import ast def convert_text_list(texts): texts = ast.literal_eval(texts) return [text for text in texts] TWEET_DATA[\"tweet_list\"] = TWEET_DATA[\"tweet\"].apply(convert_text_list) print(TWEET_DATA[\"tweet_list\"][90]) print(\"\\ntype : \", type(TWEET_DATA[\"tweet_list\"][90])) #### Term Frequency (TF) Setelah me - load data diatas selanjutnya membuat TF ( * Term Frequency * ), dimana yang sudah dijelaskan diatas * Term Frequency * adlah frekuensi kemunculan term * i * pada dokumen * j * dibagi dengan total dari * term * pada documen * j *. def calc_TF(document): # Counts the number of times the word appears in review TF_dict = {} for term in document: if term in TF_dict: TF_dict[term] += 1 else: TF_dict[term] = 1 # Computes tf for each word for term in TF_dict: TF_dict[term] = TF_dict[term] / len(document) return TF_dict TWEET_DATA[\"TF_dict\"] = TWEET_DATA['tweet_list'].apply(calc_TF) TWEET_DATA[\"TF_dict\"].head() Check TF result \u00b6 index = 90 print('%20s' % \"term\", \"\\t\", \"TF\\n\") for key in TWEET_DATA[\"TF_dict\"][index]: print('%20s' % key, \"\\t\", TWEET_DATA[\"TF_dict\"][index][key]) #### Inverse Document Frequency (IDF) IDF berfungsi mengurangi bobot suatu *term* jika sering muncul dan tersebar diseruluh dokumen, kemudian IDF dapat dituliskan dalam persamaan idf(t,D)=log\u2061N|{d\u2208D:t\u2208d}| Dimana N menunjukan jumlah total dokumen dalam suatu corpus, N = |D|. |{d \u2208 D : t \u2208 d}| = df(t), merupakan jumlah dokumen yang mengandung term *t*. Dalam bentuk yang lain, IDF dapat dituliskan dengan persamaan, idf(t,D)=log\u22c5\u2061(Ndf(t)+1) Penambahan 1 untuk menghidarkan pembagian terhadap 0 jika *df(t)* tidak ditemukan di dalam corpus. def calc_DF(tfDict): count_DF = {} # Run through each document's tf dictionary and increment countDict's (term, doc) pair for document in tfDict: for term in document: if term in count_DF: count_DF[term] += 1 else: count_DF[term] = 1 return count_DF DF = calc_DF(TWEET_DATA[\"TF_dict\"]) n_document = len(TWEET_DATA) def calc_IDF(__n_document, __DF): IDF_Dict = {} for term in __DF: IDF_Dict[term] = np.log(__n_document / (__DF[term] + 1)) return IDF_Dict Stores the idf dictionary \u00b6 IDF = calc_IDF(n_document, DF) calc TF-IDF \u00b6 def calc_TF_IDF(TF): TF_IDF_Dict = {} #For each word in the review, we multiply its tf and its idf. for key in TF: TF_IDF_Dict[key] = TF[key] * IDF[key] return TF_IDF_Dict Stores the TF-IDF Series \u00b6 TWEET_DATA[\"TF-IDF_dict\"] = TWEET_DATA[\"TF_dict\"].apply(calc_TF_IDF) Check TF-IDF result \u00b6 index = 90 print('%20s' % \"term\", \"\\t\", '%10s' % \"TF\", \"\\t\", '%20s' % \"TF-IDF\\n\") for key in TWEET_DATA[\"TF-IDF_dict\"][index]: print('%20s' % key, \"\\t\", TWEET_DATA[\"TF_dict\"][index][key] ,\"\\t\" , TWEET_DATA[\"TF-IDF_dict\"][index][key]) # sort descending by value for DF dictionary sorted_DF = sorted(DF.items(), key=lambda kv: kv[1], reverse=True)[:50] # Create a list of unique words from sorted dictionay sorted_DF unique_term = [item[0] for item in sorted_DF] def calc_TF_IDF_Vec(__TF_IDF_Dict): TF_IDF_vector = [0.0] * len(unique_term) # For each unique word , if it is in the review , store its TF - IDF value . for i , term in enumerate ( unique_term ) : if term in __TF_IDF_Dict : TF_IDF_vector [ i ] = __TF_IDF_Dict [ term ] return TF_IDF_vector TWEET_DATA[\"TF_IDF_Vec\"] = TWEET_DATA[\"TF-IDF_dict\"].apply(calc_TF_IDF_Vec) print(\"print first row matrix TF_IDF_Vec Series\\n\") print(TWEET_DATA[\"TF_IDF_Vec\"][0]) print(\"\\nmatrix size : \", len(TWEET_DATA[\"TF_IDF_Vec\"][0])) # Convert Series to List TF_IDF_Vec_List = np.array(TWEET_DATA[\"TF_IDF_Vec\"].to_list()) # Sum element vector in axis=0 sums = TF_IDF_Vec_List.sum(axis=0) data = [] for col, term in enumerate(unique_term): data.append((term, sums[col])) ranking = pd.DataFrame(data, columns=['term', 'rank']) ranking.sort_values('rank', ascending=False) ``` Save TF-IDF Data \u00b6 Save ke CSV \u00b6 TWEET_DATA.to_csv(\"Text_TF-IDF.csv\") Save ke Excel \u00b6 TWEET_DATA.to_excel(\"Text_TF-IDF.xlsx\") Save ke HDF5 \u00b6 TWEET_DATA.to_hdf(\"Text_TF-IDF.h5\")","title":"TF-IDF (Materi 3)"},{"location":"TF-IDF/#tf-idf_materi3","text":"TF-IDF adalah ukuran statistik yang menggambarkan pentingnya suatu istilah terhadap sebuah dokumen dalam sebuah kumpulan atau korpus 1. ukuran ini sering digunakan sebagai faktor pembobot dalam pencarian temu balik informasi (information retrieval), penambangan text (text mining), dan pemodelan pengguna (user-modeling). Dalam TF-IDF Term frequency menyatakan fekuensi (tingkat keseringan) munculnya seuatu term dalam dokumen, sedangkan document frequency merupakan banyaknya jumlah dokumen dimana sebuah term itu muncul2.","title":"TF-IDF (materi3)"},{"location":"TF-IDF/#proses_tf-idf","text":"Siapakan data yang akan diolah, baca file menggunakan Pandas. Kolom yang dibutuhkan dalam pemrosesan kali ini adalah kolom label dan tweet_tokens_stemmed . Kemudian rename kolom pada Dataframe menjadi [\"label\", \"tweet\"] ```import pandas as pd import pandas as pd import numpy as np TWEET_DATA = pd.read_csv(\"Text_Preprocessing.csv\", usecols=[\"label\", \"tweet_tokens_stemmed\"]) TWEET_DATA.columns = [\"label\", \"tweet\"] TWEET_DATA.head()","title":"Proses TF-IDF"},{"location":"TF-IDF/#convert_list_formated_string_to_list","text":"import ast def convert_text_list(texts): texts = ast.literal_eval(texts) return [text for text in texts] TWEET_DATA[\"tweet_list\"] = TWEET_DATA[\"tweet\"].apply(convert_text_list) print(TWEET_DATA[\"tweet_list\"][90]) print(\"\\ntype : \", type(TWEET_DATA[\"tweet_list\"][90]))","title":"convert list formated string to list"},{"location":"TF-IDF/#convert_list_formated_string_to_list_jika_ketika_kita_cek_item_di_kolom_tweet_memiliki_tipe_data","text":"","title":"convert list formated string to list Jika ketika kita cek, item di kolom tweet memiliki tipe data  ,"},{"location":"TF-IDF/#maka_kita_perlu_mengubah_string_menjadi_list_dan_menyimpan_pada_pandas_seriec_tweet_list","text":"import ast def convert_text_list(texts): texts = ast.literal_eval(texts) return [text for text in texts] TWEET_DATA[\"tweet_list\"] = TWEET_DATA[\"tweet\"].apply(convert_text_list) print(TWEET_DATA[\"tweet_list\"][90]) print(\"\\ntype : \", type(TWEET_DATA[\"tweet_list\"][90])) #### Term Frequency (TF) Setelah me - load data diatas selanjutnya membuat TF ( * Term Frequency * ), dimana yang sudah dijelaskan diatas * Term Frequency * adlah frekuensi kemunculan term * i * pada dokumen * j * dibagi dengan total dari * term * pada documen * j *. def calc_TF(document): # Counts the number of times the word appears in review TF_dict = {} for term in document: if term in TF_dict: TF_dict[term] += 1 else: TF_dict[term] = 1 # Computes tf for each word for term in TF_dict: TF_dict[term] = TF_dict[term] / len(document) return TF_dict TWEET_DATA[\"TF_dict\"] = TWEET_DATA['tweet_list'].apply(calc_TF) TWEET_DATA[\"TF_dict\"].head()","title":"maka kita perlu mengubah string menjadi list dan menyimpan pada Pandas Seriec tweet_list ,"},{"location":"TF-IDF/#check_tf_result","text":"index = 90 print('%20s' % \"term\", \"\\t\", \"TF\\n\") for key in TWEET_DATA[\"TF_dict\"][index]: print('%20s' % key, \"\\t\", TWEET_DATA[\"TF_dict\"][index][key]) #### Inverse Document Frequency (IDF) IDF berfungsi mengurangi bobot suatu *term* jika sering muncul dan tersebar diseruluh dokumen, kemudian IDF dapat dituliskan dalam persamaan idf(t,D)=log\u2061N|{d\u2208D:t\u2208d}| Dimana N menunjukan jumlah total dokumen dalam suatu corpus, N = |D|. |{d \u2208 D : t \u2208 d}| = df(t), merupakan jumlah dokumen yang mengandung term *t*. Dalam bentuk yang lain, IDF dapat dituliskan dengan persamaan, idf(t,D)=log\u22c5\u2061(Ndf(t)+1) Penambahan 1 untuk menghidarkan pembagian terhadap 0 jika *df(t)* tidak ditemukan di dalam corpus. def calc_DF(tfDict): count_DF = {} # Run through each document's tf dictionary and increment countDict's (term, doc) pair for document in tfDict: for term in document: if term in count_DF: count_DF[term] += 1 else: count_DF[term] = 1 return count_DF DF = calc_DF(TWEET_DATA[\"TF_dict\"]) n_document = len(TWEET_DATA) def calc_IDF(__n_document, __DF): IDF_Dict = {} for term in __DF: IDF_Dict[term] = np.log(__n_document / (__DF[term] + 1)) return IDF_Dict","title":"Check TF result"},{"location":"TF-IDF/#stores_the_idf_dictionary","text":"IDF = calc_IDF(n_document, DF)","title":"Stores the idf dictionary"},{"location":"TF-IDF/#calc_tf-idf","text":"def calc_TF_IDF(TF): TF_IDF_Dict = {} #For each word in the review, we multiply its tf and its idf. for key in TF: TF_IDF_Dict[key] = TF[key] * IDF[key] return TF_IDF_Dict","title":"calc TF-IDF"},{"location":"TF-IDF/#stores_the_tf-idf_series","text":"TWEET_DATA[\"TF-IDF_dict\"] = TWEET_DATA[\"TF_dict\"].apply(calc_TF_IDF)","title":"Stores the TF-IDF Series"},{"location":"TF-IDF/#check_tf-idf_result","text":"index = 90 print('%20s' % \"term\", \"\\t\", '%10s' % \"TF\", \"\\t\", '%20s' % \"TF-IDF\\n\") for key in TWEET_DATA[\"TF-IDF_dict\"][index]: print('%20s' % key, \"\\t\", TWEET_DATA[\"TF_dict\"][index][key] ,\"\\t\" , TWEET_DATA[\"TF-IDF_dict\"][index][key]) # sort descending by value for DF dictionary sorted_DF = sorted(DF.items(), key=lambda kv: kv[1], reverse=True)[:50] # Create a list of unique words from sorted dictionay sorted_DF unique_term = [item[0] for item in sorted_DF] def calc_TF_IDF_Vec(__TF_IDF_Dict): TF_IDF_vector = [0.0] * len(unique_term) # For each unique word , if it is in the review , store its TF - IDF value . for i , term in enumerate ( unique_term ) : if term in __TF_IDF_Dict : TF_IDF_vector [ i ] = __TF_IDF_Dict [ term ] return TF_IDF_vector TWEET_DATA[\"TF_IDF_Vec\"] = TWEET_DATA[\"TF-IDF_dict\"].apply(calc_TF_IDF_Vec) print(\"print first row matrix TF_IDF_Vec Series\\n\") print(TWEET_DATA[\"TF_IDF_Vec\"][0]) print(\"\\nmatrix size : \", len(TWEET_DATA[\"TF_IDF_Vec\"][0])) # Convert Series to List TF_IDF_Vec_List = np.array(TWEET_DATA[\"TF_IDF_Vec\"].to_list()) # Sum element vector in axis=0 sums = TF_IDF_Vec_List.sum(axis=0) data = [] for col, term in enumerate(unique_term): data.append((term, sums[col])) ranking = pd.DataFrame(data, columns=['term', 'rank']) ranking.sort_values('rank', ascending=False) ```","title":"Check TF-IDF result"},{"location":"TF-IDF/#save_tf-idf_data","text":"","title":"Save TF-IDF Data"},{"location":"TF-IDF/#save_ke_csv","text":"TWEET_DATA.to_csv(\"Text_TF-IDF.csv\")","title":"Save ke CSV"},{"location":"TF-IDF/#save_ke_excel","text":"TWEET_DATA.to_excel(\"Text_TF-IDF.xlsx\")","title":"Save ke Excel"},{"location":"TF-IDF/#save_ke_hdf5","text":"TWEET_DATA.to_hdf(\"Text_TF-IDF.h5\")","title":"Save ke HDF5"},{"location":"license/","text":"License \u00b6 MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY Support Author \u00b6 Amazon wish list","title":"License"},{"location":"license/#license","text":"MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY","title":"License"},{"location":"license/#support_author","text":"Amazon wish list","title":"Support Author"},{"location":"material-for-mkdocs/","text":"Material for MkDocs \u00b6 MkDocs \u00b6 mkdocs/mkdocs: Project documentation with Markdown - GitHub Material for MkDocs \u00b6 squidfunk/mkdocs-material: A Material Design theme for MkDocs","title":"Material for MkDocs"},{"location":"material-for-mkdocs/#material_for_mkdocs","text":"","title":"Material for MkDocs"},{"location":"material-for-mkdocs/#mkdocs","text":"mkdocs/mkdocs: Project documentation with Markdown - GitHub","title":"MkDocs"},{"location":"material-for-mkdocs/#material_for_mkdocs_1","text":"squidfunk/mkdocs-material: A Material Design theme for MkDocs","title":"Material for MkDocs"},{"location":"extensions/code-hilite/","text":"CodeHilite \u00b6 CodeHilite - Material for MkDocs Supported languages - Pygments Configure mkdocs.yml \u00b6 markdown_extensions: - codehilite","title":"CodeHilite"},{"location":"extensions/code-hilite/#codehilite","text":"CodeHilite - Material for MkDocs Supported languages - Pygments","title":"CodeHilite"},{"location":"extensions/code-hilite/#configure_mkdocsyml","text":"markdown_extensions: - codehilite","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/","text":"Footnote \u00b6 Footnotes - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - footnotes Example \u00b6 Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Footnote"},{"location":"extensions/footnote/#footnote","text":"Footnotes - Material for MkDocs","title":"Footnote"},{"location":"extensions/footnote/#configure_mkdocsyml","text":"markdown_extensions: - footnotes","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/#example","text":"Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Example"},{"location":"extensions/mathjax/","text":"MathJax \u00b6 PyMdown - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - mdx_math: enable_dollar_delimiter: True Example code \u00b6 $$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$ Example rendering \u00b6 P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha","title":"MathJax"},{"location":"extensions/mathjax/#mathjax","text":"PyMdown - Material for MkDocs","title":"MathJax"},{"location":"extensions/mathjax/#configure_mkdocsyml","text":"markdown_extensions: - mdx_math: enable_dollar_delimiter: True","title":"Configure mkdocs.yml"},{"location":"extensions/mathjax/#example_code","text":"$$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$","title":"Example code"},{"location":"extensions/mathjax/#example_rendering","text":"P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha","title":"Example rendering"}]}